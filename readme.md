2020.02.02
# BERT (Oct 2018) is good for: 
- [2019.11 ChrisMcCormickAI: BERT Research - Ep. 1 - Key Concepts & Sources](https://www.youtube.com/watch?v=FKlPCK1uFrc&list=PLam9sigHPGwOBuH4_4fr-XvDbe5uneaf6)<br>
    - BERT <- Transformer <- ]LSTM w/ Attention] <- [Encoder/Decoder + Bi-LSTM] <- [RNN + LSTM]
    - Bogus tasks: (1) Masked Language Model, (2) Next Sentence Prediction
- [Blog: http://mccormickml.com/2019/11/11/bert](http://mccormickml.com/2019/11/11/bert-research-ep-1-key-concepts-and-sources/)<br>
- [Jay Alammar: The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning](http://jalammar.github.io/illustrated-bert/)<br>
[google search results](https://www.google.com/search?q=BERT+and+other+transformers&rlz=1C1GCEA_enUS800US800&oq=bert&aqs=chrome.2.69i57j0j69i59j46l2j69i64l3.5639j0j7&sourceid=chrome&ie=UTF-8)<br>
- [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding
Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova](https://arxiv.org/abs/1810.04805)<br>
- [The Annotated Transformer](https://nlp.seas.harvard.edu/2018/04/03/attention.html)<br>

[]()<br>
[]()<br>

# Summarization
[EVALUATION MEASURES FOR TEXT SUMMARIZATION, J.Steinberger et al, Computing and Informatics, Vol. 28, 2009, 1001â€“1026, V 2009-Mar-2 ](http://www.cai.sk/ojs/index.php/cai/article/viewFile/37/24)<br>
[]()<br>
[]()<br>
[]()<br>
[]()<br>
[]()<br>
[]()<br>
[]()<br>
